{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def rm_tags(text):\n",
    "    re_tag = re.compile(r'<[^>]+>')\n",
    "    return re_tag.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def read_files(filetype):\n",
    "    path = \"data/aclImdb/\"\n",
    "    file_list=[]\n",
    "    \n",
    "    positive_path=path + filetype + \"/pos/\"\n",
    "    for f in os.listdir(positive_path):\n",
    "        file_list+=[positive_path+f]\n",
    "        \n",
    "    negative_path=path + filetype + \"/neg/\"\n",
    "    for f in os.listdir(negative_path):\n",
    "        file_list+=[negative_path+f]\n",
    "    \n",
    "    print('read',filetype, 'files:',len(file_list))\n",
    "    \n",
    "    all_labels = ([1] * 12500 + [0] * 12500)\n",
    "    \n",
    "    all_texts = []\n",
    "    for fi in file_list:\n",
    "        with open(fi, encoding = 'utf8') as file_input:\n",
    "            all_texts += [rm_tags(\" \".join(file_input.readlines()))]\n",
    "            \n",
    "    return all_labels,all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read train files: 25000\n"
     ]
    }
   ],
   "source": [
    "y_train,train_text=read_files(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read test files: 25000\n"
     ]
    }
   ],
   "source": [
    "y_test,test_text=read_files('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(num_words=3800)\n",
    "token.fit_on_texts(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_seq = token.texts_to_sequences(train_text)\n",
    "x_test_seq = token.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train_seq, maxlen=380)\n",
    "x_test  = sequence.pad_sequences(x_test_seq,  maxlen=380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation,Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(output_dim=32,\n",
    "                    input_dim=3800,\n",
    "                    input_length=380))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=256,\n",
    "               activation='relu'))\n",
    "model.add(Dropout(0.35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=1,\n",
    "               activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 380, 32)           121600    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 380, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12160)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               3113216   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,235,073\n",
      "Trainable params: 3,235,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      " - 32s - loss: 0.4732 - acc: 0.7591 - val_loss: 0.3492 - val_acc: 0.8508\n",
      "Epoch 2/10\n",
      " - 31s - loss: 0.2008 - acc: 0.9215 - val_loss: 0.4287 - val_acc: 0.8288\n",
      "Epoch 3/10\n",
      " - 33s - loss: 0.0769 - acc: 0.9769 - val_loss: 0.7124 - val_acc: 0.7758\n",
      "Epoch 4/10\n",
      " - 39s - loss: 0.0280 - acc: 0.9923 - val_loss: 0.5238 - val_acc: 0.8612\n",
      "Epoch 5/10\n",
      " - 36s - loss: 0.0158 - acc: 0.9959 - val_loss: 0.8162 - val_acc: 0.8090\n",
      "Epoch 6/10\n",
      " - 37s - loss: 0.0111 - acc: 0.9968 - val_loss: 1.0496 - val_acc: 0.7806\n",
      "Epoch 7/10\n",
      " - 36s - loss: 0.0096 - acc: 0.9974 - val_loss: 0.9755 - val_acc: 0.8090\n",
      "Epoch 8/10\n",
      " - 37s - loss: 0.0117 - acc: 0.9962 - val_loss: 0.9495 - val_acc: 0.8078\n",
      "Epoch 9/10\n",
      " - 40s - loss: 0.0157 - acc: 0.9943 - val_loss: 1.1557 - val_acc: 0.7870\n",
      "Epoch 10/10\n",
      " - 35s - loss: 0.0165 - acc: 0.9946 - val_loss: 1.1992 - val_acc: 0.7900\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(x_train, y_train, batch_size=100,\n",
    "                         epochs=10,verbose=2,\n",
    "                         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 8s 333us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84467999999999999"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict=model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_classes=predict.reshape(-1)\n",
    "predict_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SentimentDict={1:'正面的',0:'負面的'}\n",
    "def display_test_Sentiment(i):\n",
    "    print(test_text[i])\n",
    "    print('label真實值:',SentimentDict[y_test[i]],\n",
    "         '預測結果:',SentimentDict[predict_classes[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor turned director Bill Paxton follows up his promising debut, the Gothic-horror \"Frailty\", with this family friendly sports drama about the 1913 U.S. Open where a young American caddy rises from his humble background to play against his Bristish idol in what was dubbed as \"The Greatest Game Ever Played.\" I'm no fan of golf, and these scrappy underdog sports flicks are a dime a dozen (most recently done to grand effect with \"Miracle\" and \"Cinderella Man\"), but some how this film was enthralling all the same.The film starts with some creative opening credits (imagine a Disneyfied version of the animated opening credits of HBO's \"Carnivale\" and \"Rome\"), but lumbers along slowly for its first by-the-numbers hour. Once the action moves to the U.S. Open things pick up very well. Paxton does a nice job and shows a knack for effective directorial flourishes (I loved the rain-soaked montage of the action on day two of the open) that propel the plot further or add some unexpected psychological depth to the proceedings. There's some compelling character development when the British Harry Vardon is haunted by images of the aristocrats in black suits and top hats who destroyed his family cottage as a child to make way for a golf course. He also does a good job of visually depicting what goes on in the players' heads under pressure. Golf, a painfully boring sport, is brought vividly alive here. Credit should also be given the set designers and costume department for creating an engaging period-piece atmosphere of London and Boston at the beginning of the twentieth century.You know how this is going to end not only because it's based on a true story but also because films in this genre follow the same template over and over, but Paxton puts on a better than average show and perhaps indicates more talent behind the camera than he ever had in front of it. Despite the formulaic nature, this is a nice and easy film to root for that deserves to find an audience.\n",
      "label真實值: 正面的 預測結果: 正面的\n"
     ]
    }
   ],
   "source": [
    "display_test_Sentiment(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First of all I hate those moronic rappers, who could'nt act if they had a gun pressed against their foreheads. All they do is curse and shoot each other and acting like cliché'e version of gangsters.The movie doesn't take more than five minutes to explain what is going on before we're already at the warehouse There is not a single sympathetic character in this movie, except for the homeless guy, who is also the only one with half a brain.Bill Paxton and William Sadler are both hill billies and Sadlers character is just as much a villain as the gangsters. I did'nt like him right from the start.The movie is filled with pointless violence and Walter Hills specialty: people falling through windows with glass flying everywhere. There is pretty much no plot and it is a big problem when you root for no-one. Everybody dies, except from Paxton and the homeless guy and everybody get what they deserve.The only two black people that can act is the homeless guy and the junkie but they're actors by profession, not annoying ugly brain dead rappers.Stay away from this crap and watch 48 hours 1 and 2 instead. At lest they have characters you care about, a sense of humor and nothing but real actors in the cast.\n",
      "label真實值: 負面的 預測結果: 負面的\n"
     ]
    }
   ],
   "source": [
    "display_test_Sentiment(12502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_text='''\n",
    "I am a huge fan of the original and I was thrilled when the cast was announced. I'm a big fan of Emma Watson and most of the other supporting actors so I went in with high hopes for this. It was awful! The CGI and auto-tuning were distracting and poorly done. On the subject of auto-tune- why did they insist Emma Watson do her own vocals when she clearly wasn't up to the task? Several other numbers, notably \"Gaston\" and \"Be our Guest\" fell flat. None of the charm or warmth of the original.\n",
    "The performances were another issue for me which blows my mind considering the talent propping this horrid remake up. I can't fathom who approved the accents of Ewen Mcgregor and Emma Thompson. They were BAD. Emma Watson's performance was not what I expected from her. It was like she was trying but missing the mark time and again. Her Belle is condescending at times, bland in others, and overall forgettable. **spoiler** When Gaston and the beast have their fight,in this version instead of stabbing beast in the back, Gaston shoots him unexpectedly like twice. Emma Watson's \"reaction\" to this is a prime example of my above complaints. She doesn't seem shocked, sad, NOTHING. She waits until he's been shot a few times and has been down awhile before changing emotion at all and even then her \"sorrow\" at his death is horribly unbelievable. I could not believe this was Emma Watson preforming in this way. The beast was eh, Lafou wasn't funny (the theater was at no point filled with laughter. My 10 year old laughed twice the whole time), and the servants weren't charming or at all like their cartoon versions. I also hate that the funny back and forth between Lumiere and Cogsworth wasn't there. The only one I enjoyed was Luke Evans as Gaston. He was far from perfect but I think he did best out of everyone. \n",
    "As I scroll through the IMDb reviews with the occasional 8 or 9, and pages of 2's and 4's, I can't understand how the rating is a 7.8. I give it a 2 for effort and can say with 100% certainty that I won't ever sit through it again. Another pointless remake. Disappointing\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_seq = token.texts_to_sequences([input_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 240, 3, 662, 333, 4, 1, 200, 2, 9, 12, 50, 1, 173, 12, 142, 3, 190, 333, 4, 2637, 2, 87, 4, 1, 81, 692, 152, 34, 9, 431, 7, 15, 308, 1905, 14, 10, 8, 12, 369, 1, 1679, 2, 67, 2, 857, 220, 19, 1, 871, 4, 3191, 134, 118, 32, 2637, 78, 37, 201, 50, 55, 691, 281, 52, 5, 1, 2781, 446, 81, 1390, 3715, 2, 25, 259, 3463, 1579, 1030, 595, 4, 1, 1375, 38, 4, 1, 200, 1, 350, 67, 155, 1828, 14, 68, 59, 3660, 57, 326, 1065, 1, 671, 10, 1029, 52, 9, 187, 33, 1, 2459, 4, 2, 2637, 32, 67, 75, 2637, 235, 12, 20, 47, 9, 868, 35, 37, 8, 12, 36, 55, 12, 265, 17, 1008, 1, 947, 54, 2, 170, 37, 6, 29, 207, 1914, 7, 404, 2, 442, 2442, 1379, 50, 2, 1, 2772, 24, 64, 544, 7, 10, 306, 300, 4, 2772, 7, 1, 141, 3239, 86, 36, 1447, 2637, 2087, 5, 10, 6, 3, 2478, 456, 4, 57, 748, 55, 148, 302, 2412, 614, 159, 55, 362, 236, 73, 320, 3, 167, 207, 2, 43, 73, 176, 154, 2535, 1421, 29, 28, 2, 56, 91, 37, 29, 23, 337, 6, 2356, 1296, 9, 96, 20, 260, 10, 12, 2637, 7, 10, 92, 1, 2772, 12, 281, 158, 1, 747, 12, 29, 53, 209, 1058, 15, 2125, 57, 160, 287, 150, 1505, 1447, 1, 222, 54, 2, 1, 1169, 1217, 38, 29, 28, 36, 64, 1068, 2054, 9, 77, 779, 11, 1, 158, 141, 2, 2576, 196, 2, 281, 46, 1, 60, 27, 9, 506, 12, 2171, 13, 26, 12, 226, 35, 400, 17, 9, 100, 26, 118, 114, 42, 4, 312, 13, 9, 139, 1, 894, 852, 15, 1, 2550, 708, 38, 788, 2, 4, 2, 9, 187, 387, 85, 1, 673, 6, 3, 689, 708, 9, 198, 8, 3, 237, 14, 776, 2, 66, 131, 15, 1240, 11, 9, 524, 122, 865, 139, 8, 170, 155, 1146, 1029, 1327]\n"
     ]
    }
   ],
   "source": [
    "print(input_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad_input_seq = sequence.pad_sequences(input_seq, maxlen=380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pad_input_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_result=model.predict_classes(pad_input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'正面的'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentDict[predict_result[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_review(input_text):\n",
    "    input_seq = token.texts_to_sequences([input_text])\n",
    "    pad_input_seq = sequence.pad_sequences(input_seq, maxlen=380)\n",
    "    predict_result=model.predict_classes(pad_input_seq)\n",
    "    print(SentimentDict[predict_result[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "負面的\n"
     ]
    }
   ],
   "source": [
    "predict_review('''\n",
    "THIS MAY CONTAIN SPOILERS \n",
    "\n",
    "It is really simple. \n",
    "\n",
    "If they wanted to make a good movie, a really good movie they would take care of two things. 1. Belle. They would cast an actor who can actually ...act.\n",
    "\n",
    "(After watching the movie it is pretty obvious that the only reason Watson landed the role is her pretty big fan base of teenagers from her Harry Potter films)\n",
    "\n",
    "What we needed for Belle's role? \n",
    "\n",
    "Fresh, energetic, with a smile that lights her face actress. \n",
    "\n",
    "What we got? \n",
    "\n",
    "The twin sister of Bella Swan that landed herself in medieval France. Seriously I was waiting for Edward Cullen to do a cameo from time to time. \n",
    "\n",
    "Haven't watched miss Watson in Harry Potter films but this was just a disaster. \n",
    "\n",
    "2. The Beast. \n",
    "\n",
    "It was so.... distressing to watch a soulless face throughout the movie talking. Even at the peak of the Beast's story when he lets Belle go, and we suppose to see the agony, the pain, all I get is.... nothing. Soulless. \n",
    "\n",
    "Flat. \n",
    "\n",
    "Disappointing.\n",
    "\n",
    "I am confused. Should the actor still work harder and deliver his role as a beast better or it is due that horrible CGI head that we can't appreciate his performance? \n",
    "\n",
    "Either way, again, they could do a lot better. \n",
    "\n",
    "To sum this up, soulless Beast x miscast Belle leads to this awful remake to a classic animated film that we all grew up to love. \n",
    "\n",
    "Save your money and go watch something else.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
